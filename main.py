__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import os
import time
import functools
import uuid
import logging
import requests
import zipfile

import streamlit as st
import streamlit.components.v1 as components
from dotenv import load_dotenv

import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from langchain_chroma import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_community.document_transformers import LongContextReorder
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_openai import ChatOpenAI

# ë¡œê·¸ ë ˆë²¨ ê°ì†Œ
logging.basicConfig(level=logging.WARNING)

load_dotenv()

# â€”â€”â€” ğŸš€ Google Drive ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ â€”â€”â€”
@st.cache_data
def download_and_extract_databases():
    """Google Driveì—ì„œ ChromaDB íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì••ì¶• í•´ì œ"""
    
    # ì••ì¶• íŒŒì¼ ì •ë³´ (íŒŒì¼ëª…, ì¶”ì¶œ ìœ„ì¹˜, Google Drive íŒŒì¼ ID)
    files_to_download = [
        {
            "filename": "chroma_db_law_real_final.zip",
            "extract_dir": "chroma_db_law_real_final",
            "gdrive_id": "1gp5h0QScWB3wcsbs4i12ny1wEMY_HAqX"
        },
        {
            "filename": "ja_chroma_db.zip", 
            "extract_dir": "ja_chroma_db",
            "gdrive_id": "1dU9TLAPMg-Q8DLQjZM38CC-TsK477dSO"
        }
    ]

    def download_and_extract_single(file_info):
        url = f"https://drive.google.com/uc?export=download&id={file_info['gdrive_id']}"
        zip_path = file_info["filename"]
        extract_path = file_info["extract_dir"]

        if not os.path.exists(extract_path):
            st.info(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {zip_path}")
            
            # ë‹¤ìš´ë¡œë“œ ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                # requestsë¡œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                status_text.text("ì„œë²„ ì—°ê²° ì¤‘...")
                progress_bar.progress(10)
                
                r = requests.get(url, stream=True)
                r.raise_for_status()
                
                total_size = int(r.headers.get('content-length', 0))
                downloaded_size = 0
                
                status_text.text(f"ë‹¤ìš´ë¡œë“œ ì¤‘: {zip_path}")
                
                with open(zip_path, "wb") as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded_size += len(chunk)
                            if total_size > 0:
                                progress = min(50, int((downloaded_size / total_size) * 40) + 10)
                                progress_bar.progress(progress)

                progress_bar.progress(60)
                status_text.text(f"ì••ì¶• í•´ì œ ì¤‘: {zip_path}")

                # ì••ì¶• í•´ì œ
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_path)
                
                progress_bar.progress(90)
                
                # ì••ì¶• íŒŒì¼ ì‚­ì œ (ìš©ëŸ‰ ì ˆì•½)
                os.remove(zip_path)
                
                progress_bar.progress(100)
                status_text.text(f"âœ… ì™„ë£Œ: {extract_path}")
                
                # UI ì •ë¦¬
                time.sleep(1)
                progress_bar.empty()
                status_text.empty()
                
                st.success(f"âœ… {extract_path} ì¤€ë¹„ ì™„ë£Œ!")
                
            except Exception as e:
                st.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {zip_path} - {str(e)}")
                return False
        else:
            st.success(f"âœ… ì´ë¯¸ ì¡´ì¬í•¨: {extract_path}")
        
        return True

    # ëª¨ë“  íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
    all_success = True
    for file_info in files_to_download:
        success = download_and_extract_single(file_info)
        all_success = all_success and success
    
    return all_success

# â€”â€”â€” ì»¤ìŠ¤í…€ CSS ìŠ¤íƒ€ì¼ â€”â€”â€”
def load_custom_css():
    st.markdown("""
    <style>
    /* ì „ì²´ ë°°ê²½ */
    .stApp {
        background: linear-gradient(135deg, #f5f3ff 0%, #faf9ff 50%, #fffbeb 100%);
    }
    
    /* ë©”ì¸ ì»¨í…Œì´ë„ˆ */
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
        max-width: 1200px;
    }
    
    /* í—¤ë” ìŠ¤íƒ€ì¼ */
    .header-container {
        background: linear-gradient(135deg, #8b5cf6 0%, #a78bfa 50%, #c4b5fd 100%);
        padding: 2.5rem 2rem;
        border-radius: 20px;
        margin-bottom: 2rem;
        box-shadow: 0 10px 30px rgba(139, 92, 246, 0.3);
        text-align: center;
        position: relative;
        overflow: hidden;
    }
    
    .header-container::before {
        content: '';
        position: absolute;
        top: -50%;
        left: -50%;
        width: 200%;
        height: 200%;
        background: linear-gradient(45deg, transparent 30%, rgba(255,255,255,0.1) 50%, transparent 70%);
        animation: shimmer 3s infinite;
    }
    
    @keyframes shimmer {
        0% { transform: translateX(-100%) translateY(-100%) rotate(45deg); }
        100% { transform: translateX(100%) translateY(100%) rotate(45deg); }
    }
    
    .header-title {
        color: white;
        font-size: 2.5rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
        position: relative;
        z-index: 1;
    }
    
    .header-subtitle {
        color: #fef3c7;
        font-size: 1.2rem;
        font-weight: 400;
        position: relative;
        z-index: 1;
    }
    
    .highlight {
        color: #fbbf24;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
    }
    
    /* ì±„íŒ… ì»¨í…Œì´ë„ˆ */
    .chat-container {
        background: transparent;
        padding: 1.5rem;
        margin: 1rem 0;
    }
    
    /* ì‚¬ìš©ì ë©”ì‹œì§€ */
    .user-message {
        display: flex;
        justify-content: flex-end;
        margin: 1rem 0;
        animation: slideInRight 0.3s ease-out;
    }
    
    .user-bubble {
        max-width: 75%;
        background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%);
        padding: 1rem 1.5rem;
        border-radius: 20px 20px 8px 20px;
        border: 2px solid #f59e0b;
        box-shadow: 0 4px 15px rgba(245, 158, 11, 0.2);
        position: relative;
    }
    
    .user-bubble::before {
        content: 'ğŸ‘¤';
        position: absolute;
        right: -0.5rem;
        top: -0.5rem;
        background: #f59e0b;
        border-radius: 50%;
        width: 2rem;
        height: 2rem;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 1rem;
    }
    
    /* AI ë©”ì‹œì§€ */
    .ai-message {
        display: flex;
        justify-content: flex-start;
        margin: 1rem 0;
        animation: slideInLeft 0.3s ease-out;
    }
    
    .ai-bubble {
        max-width: 75%;
        background: linear-gradient(135deg, #e0e7ff 0%, #c7d2fe 100%);
        padding: 1rem 1.5rem;
        border-radius: 20px 20px 20px 8px;
        border: 2px solid #8b5cf6;
        box-shadow: 0 4px 15px rgba(139, 92, 246, 0.2);
        position: relative;
    }
    
    .ai-bubble::before {
        content: 'ğŸ¤–';
        position: absolute;
        left: -0.5rem;
        top: -0.5rem;
        background: #8b5cf6;
        border-radius: 50%;
        width: 2rem;
        height: 2rem;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 1rem;
    }
    
    @keyframes slideInRight {
        from { transform: translateX(50px); opacity: 0; }
        to { transform: translateX(0); opacity: 1; }
    }
    
    @keyframes slideInLeft {
        from { transform: translateX(-50px); opacity: 0; }
        to { transform: translateX(0); opacity: 1; }
    }
    
    /* ê´‘ê³  ë°°ë„ˆ */
    .ad-banner {
        background: linear-gradient(135deg, #fffbeb 0%, #fef3c7 100%);
        border: 2px solid #f59e0b;
        border-radius: 15px;
        padding: 1.5rem;
        margin: 1.5rem 0;
        box-shadow: 0 6px 20px rgba(245, 158, 11, 0.15);
        transition: transform 0.3s ease, box-shadow 0.3s ease;
    }
    
    .ad-banner:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 25px rgba(245, 158, 11, 0.25);
    }
    
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton > button {
        background: linear-gradient(135deg, #8b5cf6 0%, #a78bfa 100%);
        color: white;
        border: none;
        border-radius: 12px;
        padding: 0.5rem 1rem;
        font-weight: 600;
        transition: all 0.3s ease;
        box-shadow: 0 4px 15px rgba(139, 92, 246, 0.3);
    }
    
    .stButton > button:hover {
        background: linear-gradient(135deg, #7c3aed 0%, #8b5cf6 100%);
        transform: translateY(-2px);
        box-shadow: 0 6px 20px rgba(139, 92, 246, 0.4);
    }
    
    /* ì…ë ¥ í•„ë“œ ìŠ¤íƒ€ì¼ */
    .stTextInput > div > div > input {
        border-radius: 15px;
        border: 2px solid #c4b5fd;
        padding: 0.75rem 1rem;
        font-size: 1rem;
        transition: border-color 0.3s ease, box-shadow 0.3s ease;
    }
    
    .stTextInput > div > div > input:focus {
        border-color: #8b5cf6;
        box-shadow: 0 0 0 3px rgba(139, 92, 246, 0.1);
    }
    
    /* ì‚¬ì´ë“œë°” ì¹´ë“œ ìŠ¤íƒ€ì¼ */
    .sidebar-card {
        background: linear-gradient(135deg, #f8fafc 0%, #f1f5f9 100%);
        border-radius: 15px;
        padding: 1.5rem;
        margin: 1rem 0;
        border: 1px solid #e2e8f0;
        box-shadow: 0 4px 15px rgba(0,0,0,0.05);
    }
    
    /* ì œëª© ìŠ¤íƒ€ì¼ ê°œì„  */
    h1, h2, h3 {
        color: #6b21a8;
        font-weight: 700;
    }
    
    /* êµ¬ë¶„ì„  ìŠ¤íƒ€ì¼ */
    hr {
        border: none;
        height: 2px;
        background: linear-gradient(90deg, transparent 0%, #c4b5fd 50%, transparent 100%);
        margin: 2rem 0;
    }
    </style>
    """, unsafe_allow_html=True)

# â€”â€”â€” ì›ë³¸ ì½”ë“œì˜ ëª¨ë“  í´ë˜ìŠ¤ë“¤ (ê·¸ëŒ€ë¡œ ìœ ì§€) â€”â€”â€”

# 1. ì¼ìƒì–´ â†’ ë²•ë¥ ì–´ ì „ì²˜ë¦¬ í´ë˜ìŠ¤
class LegalQueryPreprocessor:
    """ì¼ìƒì–´ë¥¼ ë²•ë¥  ìš©ì–´ë¡œ ë³€í™˜í•˜ëŠ” ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4o",
            temperature=0.1,
            max_tokens=200,
        )
        
        self._query_cache = {}
        
        self.term_mapping = {
            "ì§‘ì£¼ì¸": "ì„ëŒ€ì¸", "ì„¸ì…ì": "ì„ì°¨ì¸", "ì „ì„¸ê¸ˆ": "ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ",
            "ë³´ì¦ê¸ˆ": "ì„ëŒ€ì°¨ë³´ì¦ê¸ˆ", "ì›”ì„¸": "ì°¨ì„", "ë°©ì„¸": "ì°¨ì„",
            "ê³„ì•½ì„œ": "ì„ëŒ€ì°¨ê³„ì•½ì„œ", "ì§‘ ë‚˜ê°€ë¼": "ëª…ë„ì²­êµ¬", "ì«“ê²¨ë‚˜ë‹¤": "ëª…ë„",
            "ëˆ ì•ˆì¤˜": "ì±„ë¬´ë¶ˆì´í–‰", "ëˆ ëª»ë°›ì•„": "ë³´ì¦ê¸ˆë°˜í™˜ì²­êµ¬", "ì‚¬ê¸°": "ì‚¬ê¸°ì£„",
            "ì†ì•˜ë‹¤": "ê¸°ë§í–‰ìœ„", "ê¹¡í†µì „ì„¸": "ì „ì„¸ì‚¬ê¸°", "ì´ì¤‘ê³„ì•½": "ì¤‘ë³µì„ëŒ€",
            "ê³ ì†Œ": "í˜•ì‚¬ê³ ë°œ", "ê³ ë°œ": "í˜•ì‚¬ê³ ë°œ", "ì†Œì†¡": "ë¯¼ì‚¬ì†Œì†¡",
            "ì¬íŒ": "ì†Œì†¡", "ë³€í˜¸ì‚¬": "ë²•ë¬´ì‚¬", "ìƒë‹´": "ë²•ë¥ ìƒë‹´",
            "í•´ê²°": "ë¶„ìŸí•´ê²°", "ë³´ìƒ": "ì†í•´ë°°ìƒ", "ë°°ìƒ": "ì†í•´ë°°ìƒ",
            "ê³„ì•½": "ë²•ë¥ í–‰ìœ„", "ì•½ì†": "ê³„ì•½", "ìœ„ë°˜": "ì±„ë¬´ë¶ˆì´í–‰", "ì–´ê¸°ë‹¤": "ìœ„ë°˜í•˜ë‹¤"
        }
    
    def _apply_rule_based_conversion(self, query: str) -> str:
        converted_query = query
        for common_term, legal_term in self.term_mapping.items():
            if common_term in converted_query:
                converted_query = converted_query.replace(common_term, legal_term)
        return converted_query
    
    def _is_already_legal_query(self, query: str) -> bool:
        legal_indicators = [
            "ì„ëŒ€ì¸", "ì„ì°¨ì¸", "ì„ëŒ€ì°¨", "ëª…ë„", "ì±„ë¬´ë¶ˆì´í–‰", 
            "ì†í•´ë°°ìƒ", "ë¯¼ì‚¬ì†Œì†¡", "í˜•ì‚¬ê³ ë°œ", "ë³´ì¦ê¸ˆë°˜í™˜",
            "ë²•ë¥ ", "íŒë¡€", "ë²•ë ¹", "ì†Œì†¡", "ê³„ì•½ì„œ"
        ]
        return any(term in query for term in legal_indicators)
    
    @functools.lru_cache(maxsize=100)
    def _gpt_convert_to_legal_terms(self, user_query: str) -> str:
        try:
            prompt = f"""ë‹¤ìŒ ì¼ìƒì–´ ì§ˆë¬¸ì„ ë²•ë¥  ê²€ìƒ‰ì— ì í•©í•œ ì „ë¬¸ ìš©ì–´ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
            ì›ë˜ ì§ˆë¬¸: {user_query}
            ë³€í™˜ ê·œì¹™:
            1. ì¼ìƒì–´ë¥¼ ì •í™•í•œ ë²•ë¥  ìš©ì–´ë¡œ ë°”ê¾¸ê¸°
            2. í•µì‹¬ ë²•ì  ìŸì ì„ ë¶€ê°ì‹œí‚¤ê¸°
            3. ê²€ìƒ‰ì— ë„ì›€ì´ ë˜ëŠ” ê´€ë ¨ ë²•ë¥  í‚¤ì›Œë“œ ì¶”ê°€
            4. ì›ë˜ ì˜ë¯¸ëŠ” ìœ ì§€í•˜ë©´ì„œ ë” ì •í™•í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ í‘œí˜„
            ë³€í™˜ëœ ê²€ìƒ‰ ì¿¼ë¦¬:"""

            messages = [{"role": "user", "content": prompt}]
            response = self.llm.invoke(messages)
            
            converted = response.content.strip()
            if "ë³€í™˜ëœ ê²€ìƒ‰ ì¿¼ë¦¬:" in converted:
                converted = converted.split("ë³€í™˜ëœ ê²€ìƒ‰ ì¿¼ë¦¬:")[-1].strip()
            
            return converted
            
        except Exception as e:
            print(f"âš ï¸ GPT ë³€í™˜ ì‹¤íŒ¨, ë£°ë² ì´ìŠ¤ ë³€í™˜ ì‚¬ìš©: {e}")
            return self._apply_rule_based_conversion(user_query)
    
    def convert_query(self, user_query: str) -> tuple[str, str]:
        try:
            if self._is_already_legal_query(user_query):
                return user_query, "no_conversion"
            
            if user_query in self._query_cache:
                return self._query_cache[user_query], "cached"
            
            rule_converted = self._apply_rule_based_conversion(user_query)
            
            if len(rule_converted) != len(user_query) or rule_converted != user_query:
                self._query_cache[user_query] = rule_converted
                return rule_converted, "rule_based"
            
            print("ğŸ”„ ì •êµí•œ ë²•ë¥  ìš©ì–´ ë³€í™˜ ì¤‘...")
            gpt_converted = self._gpt_convert_to_legal_terms(user_query)
            
            self._query_cache[user_query] = gpt_converted
            return gpt_converted, "gpt_converted"
            
        except Exception as e:
            print(f"âš ï¸ ì¿¼ë¦¬ ë³€í™˜ ì˜¤ë¥˜: {e}")
            return user_query, "error"

# 2. ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ì„ë² ë”© ëª¨ë¸ ìµœì í™”
class SingletonMeta(type):
    _instances = {}
    def __call__(cls, *args, **kwargs):
        if cls not in cls._instances:
            cls._instances[cls] = super().__call__(*args, **kwargs)
        return cls._instances[cls]

class OptimizedKoSBERTEmbeddings(metaclass=SingletonMeta):
    def __init__(self, model_name="jhgan/ko-sbert-sts"):
        if not hasattr(self, 'model'):
            print(f"ğŸ”„ KoSBERT ëª¨ë¸ ë¡œë”©: {model_name}")
            self.model = SentenceTransformer(model_name)
            print("âœ… KoSBERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    @functools.lru_cache(maxsize=128)
    def embed_query_cached(self, text):
        return tuple(self.model.encode(text))
    
    def embed_documents(self, texts):
        return self.model.encode(texts, batch_size=32)
    
    def embed_query(self, text):
        cached_result = self.embed_query_cached(text)
        return np.array(cached_result)

# 3. RAG ì‹œìŠ¤í…œ (ê°„ì†Œí™”ëœ ë²„ì „)
class OptimizedConditionalRAGSystem:
    def __init__(self):
        print("ğŸš€ ìµœì í™”ëœ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # ì¿¼ë¦¬ ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.query_preprocessor = LegalQueryPreprocessor()
        print("âœ… ë²•ë¥  ìš©ì–´ ì „ì²˜ë¦¬ê¸° ì¤€ë¹„ ì™„ë£Œ")
        
        # ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™”
        self.legal_embedding_function = OptimizedKoSBERTEmbeddings()
        print("ğŸ“Š KoSBERT 768ì°¨ì› ì„ë² ë”© ì‚¬ìš©")
        
        # ì„ê³„ê°’ ì„¤ì •
        self.legal_similarity_threshold = 0.7
        self.news_similarity_threshold = 0.6
        self.min_relevant_docs = 3
        
        # ğŸš€ ChromaDB ì—°ê²° (ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì‚¬ìš©)
        self._init_databases()
    
    def _init_databases(self):
        """ChromaDB ì´ˆê¸°í™”"""
        try:
            # ë²•ë¥  DB ì—°ê²°
            if os.path.exists("chroma_db_law_real_final"):
                self.legal_db = Chroma(
                    persist_directory="chroma_db_law_real_final",
                    collection_name="legal_db",
                    embedding_function=self.legal_embedding_function
                )
                self.legal_vector_retriever = self.legal_db.as_retriever(
                    search_type="similarity", 
                    search_kwargs={"k": 5}
                )
                print("âœ… ë²•ë¥  DB ì—°ê²° ì™„ë£Œ")
            else:
                print("âš ï¸ ë²•ë¥  DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                self.legal_db = None
                self.legal_vector_retriever = None
            
            # ë‰´ìŠ¤ DB ì—°ê²°  
            if os.path.exists("ja_chroma_db"):
                self.news_db = Chroma(
                    persist_directory="ja_chroma_db",
                    collection_name="jeonse_fraud_embedding",
                    embedding_function=self.legal_embedding_function
                )
                self.news_vector_retriever = self.news_db.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": 4}
                )
                print("âœ… ë‰´ìŠ¤ DB ì—°ê²° ì™„ë£Œ")
            else:
                print("âš ï¸ ë‰´ìŠ¤ DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                self.news_db = None
                self.news_vector_retriever = None
                
        except Exception as e:
            print(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
            self.legal_db = None
            self.news_db = None
            self.legal_vector_retriever = None
            self.news_vector_retriever = None
    
    def search_legal_db(self, query):
        """ë²•ë¥  DB ê²€ìƒ‰"""
        if self.legal_db is None:
            return [], 0.0
        
        try:
            legal_docs = self.legal_vector_retriever.invoke(query)
            print(f"ğŸ“„ ë²•ë¥  ê²€ìƒ‰ ê²°ê³¼: {len(legal_docs)}ê°œ ë¬¸ì„œ")
            return legal_docs, 0.8  # ê°„ë‹¨í•œ ê³ ì • ì ìˆ˜
        except Exception as e:
            print(f"âŒ ë²•ë¥  DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return [], 0.0
    
    def search_news_db(self, query):
        """ë‰´ìŠ¤ DB ê²€ìƒ‰"""
        if self.news_db is None:
            return [], 0.0
        
        try:
            news_docs = self.news_vector_retriever.invoke(query)
            print(f"ğŸ“° ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼: {len(news_docs)}ê°œ")
            return news_docs, 0.7  # ê°„ë‹¨í•œ ê³ ì • ì ìˆ˜
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return [], 0.0
    
    def conditional_retrieve(self, original_query):
        """ì¡°ê±´ë¶€ ê²€ìƒ‰"""
        try:
            print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {original_query}")
            
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬
            converted_query, conversion_method = self.query_preprocessor.convert_query(original_query)
            
            if conversion_method != "no_conversion":
                print(f"ğŸ”„ ë³€í™˜ëœ ì¿¼ë¦¬: {converted_query}")
                search_query = converted_query
            else:
                search_query = original_query
            
            # ë²•ë¥  DB ê²€ìƒ‰
            legal_docs, legal_score = self.search_legal_db(search_query)
            
            # ë‰´ìŠ¤ DB ê²€ìƒ‰
            news_docs, news_score = self.search_news_db(search_query)
            
            # ê²°ê³¼ ê²°í•©
            combined_docs = []
            if legal_docs:
                combined_docs.extend(legal_docs[:8])
            if news_docs:
                combined_docs.extend(news_docs[:3])
            
            search_type = "legal_and_news" if (legal_docs and news_docs) else ("legal_only" if legal_docs else "news_only")
            
            print(f"ğŸ¯ ìµœì¢… ê²°ê³¼: {len(combined_docs)}ê°œ ë¬¸ì„œ ({search_type})")
            return combined_docs, search_type
                
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return [], "error"

# 4. ë¬¸ì„œ í¬ë§·íŒ… (ê°„ì†Œí™”ëœ ë²„ì „)
def format_docs_optimized(docs, search_type):
    """ë¬¸ì„œ í¬ë§·íŒ…"""
    if not docs:
        return "ê´€ë ¨ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    formatted_docs = []
    for i, doc in enumerate(docs[:5]):  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
        try:
            meta = doc.metadata if doc.metadata else {}
            content = str(doc.page_content)[:500] if doc.page_content else ""
            
            is_news = ('url' in meta and 'title' in meta) or ('date' in meta and 'title' in meta)
            
            if is_news:
                title = str(meta.get("title", "ì œëª©ì—†ìŒ"))[:60]
                formatted = f"[ë‰´ìŠ¤-{i+1}] ğŸ“° {title}\në‚´ìš©: {content}...\n"
            else:
                doc_type = str(meta.get("doc_type", "ë²•ë¥ ìë£Œ"))
                formatted = f"[ë²•ë¥ -{i+1}] ğŸ›ï¸ {doc_type}\në‚´ìš©: {content}...\n"
            
            formatted_docs.append(formatted)
        except:
            continue
    
    return "\n\n".join(formatted_docs)

# 5. ì „ì—­ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
_conditional_rag = None

def get_rag_system():
    """RAG ì‹œìŠ¤í…œ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _conditional_rag
    if _conditional_rag is None:
        _conditional_rag = OptimizedConditionalRAGSystem()
    return _conditional_rag

# 6. ê²€ìƒ‰ í•¨ìˆ˜
def optimized_retrieve_and_format(query):
    """ê²€ìƒ‰ ë° í¬ë§·íŒ…"""
    try:
        rag_system = get_rag_system()
        docs, search_type = rag_system.conditional_retrieve(query)
        return format_docs_optimized(docs, search_type)
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# 7. ì±„íŒ… ì²´ì¸
def create_user_friendly_chat_chain():
    """ì‚¬ìš©ì ì¹œí™”ì  ì²´ì¸"""
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0.3,
        max_tokens=3000,
    )

    system_message = """
    ë‹¹ì‹ ì€ ë¶€ë™ì‚° ì„ëŒ€ì°¨, ì „ì„¸ì‚¬ê¸° ë“± ë²•ë¥  ë¬¸ì œë¥¼ ë•ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤.
    
    ë‹µë³€ êµ¬ì¡°:
    1. ì§ˆë¬¸ í•´ì„ ì•ˆë‚´
    2. ìœ ì‚¬ íŒë¡€ ìš”ì•½ (2ê°œ)
    3. ê´€ë ¨ ë‰´ìŠ¤ (ìˆëŠ” ê²½ìš°)
    4. í–‰ë™ë°©ì¹¨ ì œì•ˆ
    5. ìœ ì˜ì‚¬í•­
    
    ì¹œì ˆí•˜ê³  ë”°ëœ»í•œ ë§íˆ¬ë¡œ ì‹¤ì§ˆì ì¸ ë„ì›€ì„ ì œê³µí•˜ì„¸ìš”.
    """
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_message),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
        ("system", "ì°¸ê³ ìë£Œ:\n{context}")
    ])
    
    def user_friendly_retrieve_and_format(query):
        try:
            return optimized_retrieve_and_format(query)
        except Exception as e:
            return "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    chain = (
        {
            "context": RunnableLambda(lambda x: user_friendly_retrieve_and_format(x["question"])),
            "question": RunnableLambda(lambda x: x["question"]),
            "chat_history": RunnableLambda(lambda x: x.get("chat_history", [])),
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain
